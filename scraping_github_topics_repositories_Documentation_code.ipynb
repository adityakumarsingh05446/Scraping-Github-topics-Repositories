{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scraping-github-topics-repositories_Documentation_code.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMsDBccSEXLz"
      },
      "source": [
        "## **Project Title : Scraping Top Repositories for Topics on Github**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkSlY40eE-tF"
      },
      "source": [
        "## **Introduction**\n",
        "\n",
        "### **What is web scraping ?**\n",
        "Process of collecting structured web data in an automated fashion is reffered as web scraping also known as **web data extraction**.\n",
        "\n",
        "### **How does scraping works ?**\n",
        "- User creates a **search request** to **web servers**----> Servers(www.google.com) : servers than give **raw index.html** file in return ---------> User web browser like chrome gets index.html ------> Chrome beautify index.html and present it to user as a web page.\n",
        "\n",
        "- Here **index.html** file which is recieved from server for the request made by user. Index.html file is not given to browser rather then it is displayed in our **code environment** where we will scrap data via queries and will store results in CSV or excel or other database or file formats.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V283xHk0KUMY"
      },
      "source": [
        "### **Problem Statement** : \n",
        "  - Scrape the list of topics on https://github.com/topics and fetch its **topic title , topic page url and topic description**.\n",
        "  - For each topic fetch its **top 30 trending repositories** with **repository name , username , stars and its repository url.**\n",
        "  - And at last create the **topic csv files**.\n",
        "\n",
        "\n",
        "**Tools used** : Python , NumPy , requests , Beautiful Soup , Pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x_o4hTtMnpp"
      },
      "source": [
        "## **Outline**\n",
        "- As mentioned in problem statement scrape respective url i.e \" https://github.com/topics\".\n",
        "- Get all the list of topics. For each topic get its topic title , topic page url and topic description.\n",
        "- For each topic ,get all top 30 trending repositories from its topic page.\n",
        "- For each repository get its repository name , username , stars and its repository url\n",
        "- For each topic create a CSV file for it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BIAV2tc8dTX"
      },
      "source": [
        "## **Part 1 : Scraping list of topics from \" https://github.com/topics\".**\n",
        "\n",
        "**Explanation** :\n",
        "- With the use of **requests** get the index.html file from \"https://github.com/topics\".\n",
        "- Parse the html file using **html5lib** and **bs4**.\n",
        "- Using bs4 give the parse file a tree structure so that it can be used for traversal (for retrieving queries).\n",
        "- At last convert results into **pandas dataframe**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3uMzbsr-5zb"
      },
      "source": [
        "# Libraries to include\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMEbuI9GAIAy"
      },
      "source": [
        "# Funtion 1\n",
        "# Description : Parsing the https://github.com/topics page and getting a doc file from it.\n",
        "\n",
        "def get_topics_page():\n",
        "  topics_url = 'https://github.com/topics'\n",
        "  response = requests.get(topics_url)\n",
        "  # Check for successful response \n",
        "  if response.status_code != 200:\n",
        "    raise Exception('Failed to load page {}'.format(topics_url))\n",
        "  doc = BeautifulSoup(response.text , 'html.parser')\n",
        "  return doc"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "yxmE8l8nAvTx",
        "outputId": "5ce61624-6320-435d-f5a6-ed6c32e20a9d"
      },
      "source": [
        "# Funtion 1 : demo\n",
        "\n",
        "doc = get_topics_page()\n",
        "print(type(doc)) # Type of doc\n",
        "print(doc.find('a')) # Finding the first anchor tag in parsed doc."
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'bs4.BeautifulSoup'>\n",
            "<a class=\"px-2 py-4 color-bg-info-inverse color-text-white show-on-focus js-skip-to-content\" href=\"#start-of-content\">Skip to content</a>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zdMiseShIIK"
      },
      "source": [
        "**Point to be noted :** How to acquire selection_class from a page\n",
        "- Inspect the element on the web page for which information is needed to be acquired.\n",
        "- Follow the below picture\n",
        "![image.png](https://i.imgur.com/QU24yMs.png)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YdUDwGuA4E_"
      },
      "source": [
        "# Function 2\n",
        "# Description : Lets create some helper function to parse information from github topic page\n",
        "\n",
        "# Function (2.1)\n",
        "# Description : get_topic_titles() is used to retrieve the list of topics on https://github.com/topics page\n",
        "\n",
        "def get_topic_titles(doc):\n",
        "  selection_class = 'f3 lh-condensed mb-0 mt-1 Link--primary'\n",
        "  topic_title_tags = doc.find_all('p',class_=selection_class)\n",
        "  topic_titles = []\n",
        "  for tag in topic_title_tags:\n",
        "    topic_titles.append(tag.text)\n",
        "  return topic_titles"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "3SR9pDe4DFn_",
        "outputId": "24bafde5-9259-47f7-cdcf-0df1af1658a6"
      },
      "source": [
        "# Function (2.1) : demo\n",
        "topic_titles = get_topic_titles(doc)\n",
        "print(len(topic_titles))\n",
        "print(topic_titles[:5]) # Printing first 5 topics on https://github.com/topics page"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30\n",
            "['3D', 'Ajax', 'Algorithm', 'Amp', 'Android']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WVYzIe4CFkh"
      },
      "source": [
        "# Function (2.2)\n",
        "# Description : get_topic_descs() is used to get the description of the corresponding topics\n",
        "\n",
        "def get_topic_descs(doc):\n",
        "  desc_selector = 'f5 color-text-secondary mb-0 mt-1'\n",
        "  topic_desc_tags = doc.find_all('p',class_=desc_selector)\n",
        "  topic_descs = []\n",
        "  for desc in topic_desc_tags:\n",
        "    topic_descs.append(desc.text.strip())\n",
        "  return topic_descs"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "X5MG53ziilU0",
        "outputId": "c7623d53-7dca-48c5-9db7-53e9713375cb"
      },
      "source": [
        "# Function (2.2) : demo\n",
        "topic_descs = get_topic_descs(doc)\n",
        "topic_descs[0]"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'3D modeling is the process of virtually developing the surface and structure of a 3D object.'"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1M7XJVmCHXT"
      },
      "source": [
        "# Function (2.3)\n",
        "# Description : get_topic_urls() is used to get the url associated with the topics\n",
        "\n",
        "def get_topic_urls(doc):\n",
        "  topic_link_tags = doc.find_all('a',class_='d-flex no-underline')\n",
        "  topic_urls = []\n",
        "  base_url = \"https://github.com\"\n",
        "  for url in topic_link_tags:\n",
        "    topic_urls.append(base_url + url['href'])\n",
        "  return topic_urls"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LDi2oAQni3Vb",
        "outputId": "470feab3-2f0f-4d71-c072-97479da88790"
      },
      "source": [
        "# Function (2.3) : demo\n",
        "topic_urls = get_topic_urls(doc)\n",
        "topic_urls[0]"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'https://github.com/topics/3d'"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGhDW7LofDvV"
      },
      "source": [
        "# Merging the function (Function 1 and Function 2) together\n",
        "# Function 3\n",
        "# Description : This function will take list of topics , topics description and topics url and will finally convert them into a pandas dataframe\n",
        "\n",
        "\n",
        "def scrape_topics():\n",
        "  topics_url = 'https://github.com/topics'\n",
        "  response = requests.get(topics_url)\n",
        "   # Check for successful response \n",
        "  if response.status_code != 200:\n",
        "    raise Exception('Failed to load page {}'.format(topics_url))\n",
        "  topics_dict = {\n",
        "      'title':get_topic_titles(doc),\n",
        "      'description':get_topic_descs(doc),\n",
        "      'url':get_topic_urls(doc)\n",
        "  }\n",
        "  return pd.DataFrame(topics_dict)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97EM-o3xkOyG"
      },
      "source": [
        "# Function (3) : demo\n",
        "scrape_topics()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrcjyJGmk4gn"
      },
      "source": [
        "## **Part 2 : Getting the top repositories from the topics**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRXQlBDGlOGh"
      },
      "source": [
        "# Function 4\n",
        "# Description : Function which takes topic url to get topic page\n",
        "\n",
        "def get_topic_page(topic_urls):\n",
        "  # Download the page\n",
        "  response = requests.get(topic_urls)\n",
        "  # Check for successful response \n",
        "  if response.status_code != 200:\n",
        "    raise Exception('Failed to load page {}'.format(topic_urls))\n",
        "  # Parse the page using BeautifulSoup\n",
        "  topic_doc = BeautifulSoup(response.text , 'html.parser')\n",
        "  return topic_doc"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZVcRcVM7m5S"
      },
      "source": [
        " # Helper function : This function converts the stars on repository from string to integer\n",
        "def parse_star_count(stars_str):\n",
        "  stars_str = stars_str.strip()\n",
        "  if stars_str[-1] == 'k':\n",
        "    return int(float(stars_str[:-1])*1000)\n",
        "  return int(stars_str)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JwdXH9fl65-"
      },
      "source": [
        "# Function 5\n",
        "# Description : Function which takes h3 tag and star tag to grab username,repo_name,stars,repo_url.\n",
        "\n",
        "def get_repo_info(h3_tag , star_tags):\n",
        "  # Will return all the required information about the repository\n",
        "  base_url = \"https://github.com\"\n",
        "  a_tags = h3_tag.find_all('a')\n",
        "  username = a_tags[0].text.strip()\n",
        "  repo_name = a_tags[1].text.strip()\n",
        "  stars = parse_star_count(star_tags.text.strip())\n",
        "  repo_url = base_url + a_tags[1]['href']\n",
        "  return username,repo_name,stars,repo_url"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wS6-wYAYnvJZ"
      },
      "source": [
        "# Function 6\n",
        "# Description : Function to covert the topic dats's into dictionary then to pandas dataframe\n",
        "\n",
        "def get_topic_repos(topic_doc):\n",
        "  \n",
        "  # Get the h3 tag to get repo title , username and its urls\n",
        "  h3_selection_class = 'f3 color-text-secondary text-normal lh-condensed'\n",
        "  repo_tags = topic_doc.find_all('h3',class_=h3_selection_class)\n",
        "  # Get class to get star tag\n",
        "  star_tags = topic_doc.find_all('a',class_= 'social-count float-none')\n",
        "\n",
        "  # Finally get all repo info\n",
        "\n",
        "  topic_repos_dict = {\n",
        "    'username': [],\n",
        "    'repo_name':[],\n",
        "    'stars':[],\n",
        "    'repo_url':[]\n",
        "\n",
        "  }\n",
        "\n",
        "  for i in range(len(repo_tags)):\n",
        "    repo_info = get_repo_info(repo_tags[i] ,star_tags[i])\n",
        "    topic_repos_dict['username'].append(repo_info[0])\n",
        "    topic_repos_dict['repo_name'].append(repo_info[1])\n",
        "    topic_repos_dict['stars'].append(repo_info[2])\n",
        "    topic_repos_dict['repo_url'].append(repo_info[3])\n",
        "  return pd.DataFrame(topic_repos_dict)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMLk8KJnBTWq"
      },
      "source": [
        "get_topic_repos(get_topic_page(topic_urls[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qV7kHZ6vDo9E"
      },
      "source": [
        "## **Part 3: Final Merging Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Es9dLSfTnz8V"
      },
      "source": [
        " # Helper function\n",
        "import os\n",
        "def scrape_topic(topic_url , path):\n",
        "  #fname = topic_name + '.csv'\n",
        "  if os.path.exists(path):\n",
        "    print(\"The file {} already exists....skipping.. \".format(path))\n",
        "    return\n",
        "  topic_df = get_topic_repos(get_topic_page(topic_url))\n",
        "  topic_df.to_csv(path , index=None)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Xv-Wl8npvXF"
      },
      "source": [
        "# Function 7\n",
        "# Description : Final scarping function\n",
        "\n",
        "import os\n",
        "\n",
        "def scrape_topics_repos():\n",
        "  print(\"Scraping list of topics from Github\")\n",
        "  topics_df = scrape_topics()\n",
        "  # Creating a folder\n",
        "  os.makedirs('data',exist_ok=True)\n",
        "\n",
        "  for index , row in topics_df.iterrows(): # Looping over rows in pandas data frame\n",
        "    print(\"Scraping top repository for {}\".format(row['title']))\n",
        "    scrape_topic(row['url'],'data/{}.csv'.format(row['title']))"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGynjadxqxa1"
      },
      "source": [
        "scrape_topics_repos()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwpR8QnXHvuc"
      },
      "source": [
        "## **Conclusion and future works**\n",
        "- Successfully finished scraping top 30 repositories of each topic on github topic page.\n",
        "- All the data is stored into a local folder following hierarchy as \n",
        "data(folder_name)\n",
        "\n",
        "![image.png](https://i.imgur.com/N8Y7Mpd.png)\n",
        "\n",
        "- CSV file looks as \n",
        "\n",
        "![image.png](https://i.imgur.com/PtH1V0R.png)\n",
        "- Future task is comprised of finding a way to scrap data automatically from other pages as well (page 2 , page 3....)"
      ]
    }
  ]
}